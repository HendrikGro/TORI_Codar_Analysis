{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baeb21ab-683d-40f0-9cba-210cef677718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cmocean.cm as cmo\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import scipy\n",
    "from datetime import datetime,timedelta\n",
    "# from xmovie import Movie\n",
    "import nfft\n",
    "# import xrft\n",
    "import cmath\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "import math\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "# import eofs.xarray as xeof\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3366f7e1-d816-4dcf-827f-3d836d4d9ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean_xarray(da,dim):\n",
    "    return da - da.mean(dim=dim),da.mean(dim=dim)\n",
    "\n",
    "def detrend_xarray(da):\n",
    "    '''\n",
    "    apply detrend along time axis of 2D Xarray\n",
    "    see polynomial_detrend for details about detrending\n",
    "    '''\n",
    "    dt = xr.apply_ufunc(\n",
    "                polynomial_detrend,\n",
    "                da,\n",
    "                input_core_dims=[['time']],\n",
    "                output_core_dims=[['time','dim0']],\n",
    "                vectorize=True,\n",
    "                output_dtypes=[da.dtype],\n",
    "                dask=\"parallelized\",\n",
    "            )\n",
    "    if 'x' in da.dims:\n",
    "        dt = dt.transpose('time','y','x','dim0')\n",
    "    elif 'lon' in da.dims:\n",
    "        dt = dt.transpose('time','lat','lon','dim0')\n",
    "    \n",
    "    return dt.isel(dim0=0),dt.isel(dim0=1)\n",
    "\n",
    "def polynomial_detrend(da,order=1):\n",
    "    '''\n",
    "    detrend of non uniform data on a uniform grid with nan values\n",
    "    \n",
    "    change order to increase order of fittet polynomial, default is 1, so linear\n",
    "    \n",
    "    returns detrended data and trend values\n",
    "    '''\n",
    "    ds = da.copy()\n",
    "    mask = ~np.isnan(ds)\n",
    "    if mask.sum() == 0:\n",
    "        return np.stack((ds,ds),axis=-1)\n",
    "    else:\n",
    "        ds_masked = ds[mask]\n",
    "        time = np.arange(0,len(ds))\n",
    "        time_masked = time[mask]\n",
    "        coeff = np.polyfit(time_masked, ds_masked, order)\n",
    "        trend_nonan = np.polyval(coeff, time_masked)\n",
    "        detrended = ds_masked - trend_nonan\n",
    "        ds[mask] = detrended\n",
    "        trend = np.copy(ds)\n",
    "        trend[mask] = trend_nonan\n",
    "        \n",
    "        return np.stack((ds.data,trend),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b1b6119-33da-4ac2-b1ba-fc001f566fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrum1(h, dt=1):\n",
    "    \"\"\"\n",
    "    First cut at spectral estimation: very crude.\n",
    "    \n",
    "    Returns frequencies, power spectrum, and\n",
    "    power spectral density.\n",
    "    Only positive frequencies between (and not including)\n",
    "    zero and the Nyquist are output.\n",
    "    \"\"\"\n",
    "    nt = len(h)\n",
    "    npositive = nt//2\n",
    "    pslice = slice(1, npositive)\n",
    "    freqs = np.fft.fftfreq(nt, d=dt)[pslice] \n",
    "    ft = np.fft.fft(h)[pslice]\n",
    "    psraw = np.abs(ft) ** 2\n",
    "    # Double to account for the energy in the negative frequencies.\n",
    "    psraw *= 2\n",
    "    # Normalization for Power Spectrum\n",
    "    psraw /= nt**2\n",
    "    # Convert PS to Power Spectral Density\n",
    "    psdraw = psraw * dt * nt  # nt * dt is record length\n",
    "    return freqs, psraw, psdraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "597dfb15-9c6f-471e-90b9-86b37361f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nufft(data,xarray_apply=True):\n",
    "    '''\n",
    "    does a non-uniform fast fourier transform on data with a uniform grid but nan values in it\n",
    "    \n",
    "    returns freq: frequency in cycles per timestep (cph for hourly data)\n",
    "            f_k: amplitude for each wavenumber k as a complex number\n",
    "            ps: power spectrum\n",
    "            psd: power spectral density\n",
    "            \n",
    "    xarray_apply is used for the spectral_analysis function for apply_ufunc along time dimension for each point of spatial array\n",
    "        if you just want to do it for one array time series, set it to False\n",
    "    \n",
    "    function is taken from https://github.com/jakevdp/nfft\n",
    "    '''\n",
    "    mask = ~np.isnan(data)\n",
    "    \n",
    "    N_freq = len(data)\n",
    "    k = -N_freq//2 + np.arange(N_freq)\n",
    "    \n",
    "    data_masked = (data)[mask]\n",
    "    t = np.linspace(0, 1,N_freq)[mask]\n",
    "    \n",
    "    f_k = nfft.nfft_adjoint(t,data_masked,N_freq)\n",
    "    ps = np.abs(f_k)**2/N_freq**2\n",
    "    psd = ps * N_freq\n",
    "    freq = k/N_freq\n",
    "    if xarray_apply == True:\n",
    "        t_return = np.linspace(0, 1,N_freq)\n",
    "        t_return[~mask] = np.nan\n",
    "        return np.stack((freq,f_k,ps,psd,t_return),axis=-1)\n",
    "    elif xarray_apply==False:\n",
    "        return freq,f_k,ps,psd,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c50d0d7b-0351-4685-9a86-95496aaec8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_analysis(da):\n",
    "    '''\n",
    "    apply the nufft on each spatial point of a 2D dataset along the timeseries\n",
    "    returns xarray DataSet with frequency spectrum, powerspectrum, power spectral density for each point\n",
    "    t is masked time array without nans, necessary for reconstruction of time series from frequency spectrum\n",
    "    '''\n",
    "    dt = xr.apply_ufunc(\n",
    "                nufft,\n",
    "                da,\n",
    "                input_core_dims=[['time']],\n",
    "                output_core_dims=[['freq','dim0']],\n",
    "                vectorize=True,\n",
    "                output_dtypes=['complex'],\n",
    "                dask=\"parallelized\",\n",
    "                dask_gufunc_kwargs={'output_sizes':{'freq':744,'dim0':5}}\n",
    "            )\n",
    "    if 'x' in da.dims:\n",
    "        dt = xr.Dataset(coords={\n",
    "            'lat':(['y','x'],dt['lat'].data),\n",
    "            'lon':(['y','x'],dt['lon'].data),\n",
    "            'freq':np.real(dt[0,0,:,0].data)\n",
    "        },data_vars={\n",
    "            'f_k':(['y','x','freq'],dt.isel(dim0=1).data),\n",
    "            'ps':(['y','x','freq'],np.real(dt.isel(dim0=2).data)),\n",
    "            'psd':(['y','x','freq'],np.real(dt.isel(dim0=3).data)),\n",
    "            't':(['y','x','freq'],np.real(dt.isel(dim0=4).data))\n",
    "\n",
    "\n",
    "        })\n",
    "        dt = dt.transpose('freq','y','x')\n",
    "    elif 'lat' in da.dims:\n",
    "        dt = xr.Dataset(coords={\n",
    "            'lat':(dt['lat'].data),\n",
    "            'lon':(dt['lon'].data),\n",
    "            'freq':np.real(dt[0,0,:,0].data)\n",
    "        },data_vars={\n",
    "            'f_k':(['lat','lon','freq'],dt.isel(dim0=1).data),\n",
    "            'ps':(['lat','lon','freq'],np.real(dt.isel(dim0=2).data)),\n",
    "            'psd':(['lat','lon','freq'],np.real(dt.isel(dim0=3).data)),\n",
    "            't':(['lat','lon','freq'],np.real(dt.isel(dim0=4).data))\n",
    "\n",
    "\n",
    "        })\n",
    "        dt = dt.transpose('freq','lat','lon')\n",
    "    # dt = dt.where(dt!=0)\n",
    "    return dt\n",
    "\n",
    "# hann = xr.DataArray(coords={'time':velocity.time.values},data=np.hanning(744))\n",
    "# spectral_u_hann = spectral_analysis(velocity.u_demeaned*hann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7982cb-1919-45b5-b17c-9ef553143004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finder(freq_data,ps_data,freq_min,freq_max,peak_height):\n",
    "    '''\n",
    "    find amplitude and frequency of maximum peak in frequency range of spectrum\n",
    "    freq_data, ps_data: frequency values and powerspectrum\n",
    "    freq_min, freq_max: bounds of frequency range\n",
    "    peak_height: threshold for peak to detect\n",
    "    \n",
    "    if you want to detect on frequency amplitudes or power spectral density, adjust peak_height accordingly, should work\n",
    "    \n",
    "    returns maximum peak and according frequency\n",
    "    '''\n",
    "    \n",
    "    freq_mask = np.array([freq_data > freq_min]) & np.array([freq_data < freq_max])\n",
    "    freq_mask = np.squeeze(freq_mask)\n",
    "    peaks,peak_heights = scipy.signal.find_peaks(ps_data[freq_mask],height=peak_height)\n",
    "    freq_peaks = freq_data[freq_mask][peaks]\n",
    "    if len(peak_heights['peak_heights']) != 0:\n",
    "        peak_max = max(peak_heights['peak_heights'])\n",
    "        freq_max = freq_peaks[peak_heights['peak_heights'] == peak_max]\n",
    "    else:\n",
    "        peak_max,freq_max = np.nan,np.nan\n",
    "    \n",
    "    return np.array([freq_max,peak_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469f1050-7874-42f0-919c-651da7c25755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks_xr(data,freq_min,freq_max,peak_height):\n",
    "    '''\n",
    "    apply peak_finder for each point of spatial xarray\n",
    "    '''\n",
    "    ds = xr.apply_ufunc(\n",
    "                peak_finder,\n",
    "                data.freq,\n",
    "                data.ps,\n",
    "                freq_min,\n",
    "                freq_max,\n",
    "                peak_height,\n",
    "                input_core_dims=[['freq'],['freq'],[],[],[]],\n",
    "                output_core_dims=[['peak']],\n",
    "                vectorize=True,\n",
    "                output_dtypes=['float'],\n",
    "                dask=\"parallelized\",\n",
    "                dask_gufunc_kwargs={'output_sizes':{'peak':2}}\n",
    "            )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313d1949-11ee-4532-b805-ebb1ef99c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finder_rot(freq_data,ps_data,freq_min,freq_max,peak_height):\n",
    "    '''\n",
    "    find amplitude and frequency of maximum peak in frequency range of rotary spectrum\n",
    "    freq_data, ps_data: frequency values and powerspectrum\n",
    "    freq_min, freq_max: bounds of frequency range\n",
    "    peak_height: threshold for peak to detect\n",
    "    \n",
    "    if you want to detect on frequency amplitudes or power spectral density, adjust peak_height accordingly, should work\n",
    "    \n",
    "    returns maximum peak and according frequency within combined positive and negative range\n",
    "    '''\n",
    "    freq_mask = (np.array([freq_data > freq_min]) & np.array([freq_data < freq_max])) + (np.array([freq_data < -freq_min]) & np.array([freq_data > -freq_max]))\n",
    "    freq_mask = np.squeeze(freq_mask)\n",
    "    peaks,peak_heights = scipy.signal.find_peaks(ps_data[freq_mask],height=peak_height)\n",
    "    freq_peaks = freq_data[freq_mask][peaks]\n",
    "    if len(peak_heights['peak_heights']) != 0:\n",
    "        peak_max = max(peak_heights['peak_heights'])\n",
    "        freq_max = abs(freq_peaks[peak_heights['peak_heights'] == peak_max])\n",
    "    else:\n",
    "        peak_max,freq_max = np.nan,np.nan\n",
    "    \n",
    "    return np.array([freq_max,peak_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc71ab6-0781-4028-9f28-f9abba1b02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks_rot_xr(data,freq_min,freq_max,peak_height):\n",
    "    '''\n",
    "    apply peak_finder_rot for each point of spatial xarray\n",
    "    '''\n",
    "    ds = xr.apply_ufunc(\n",
    "                peak_finder_rot,\n",
    "                data.freq,\n",
    "                data.ps,\n",
    "                freq_min,\n",
    "                freq_max,\n",
    "                peak_height,\n",
    "                input_core_dims=[['freq'],['freq'],[],[],[]],\n",
    "                output_core_dims=[['peak']],\n",
    "                vectorize=True,\n",
    "                output_dtypes=['float'],\n",
    "                dask=\"parallelized\",\n",
    "                dask_gufunc_kwargs={'output_sizes':{'peak':2}}\n",
    "            )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f77a69-f631-4c52-a8e9-6fa14a017c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    '''\n",
    "    finds value and idx of value in array that is closest to the looked for value\n",
    "    '''\n",
    "    array = np.asarray(array)\n",
    "    idx = np.unravel_index(np.abs(array - value).argmin(), array.shape)\n",
    "    return array[idx],idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce06296-b8c7-40c8-a668-cbe9005299c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipse_details(fm,fp):\n",
    "    '''\n",
    "    calculates major and minor axis as well as azimuth angle of spectral ellipse from positive and negative frequency amplitudes\n",
    "    see Walden2012 for mathematical insights\n",
    "    '''\n",
    "    Rp = np.abs(fp) + np.abs(fm)\n",
    "    Rm = np.abs(np.abs(fp) - np.abs(fm))\n",
    "\n",
    "    major = np.abs(Rp + Rm)\n",
    "    minor = np.abs(Rp - Rm)\n",
    "\n",
    "    azimuth = cmath.phase(fp*fm)/2\n",
    "    return major,minor,azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a02e293-0ea3-4cc3-8c1b-b0701def2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hann_window(n,T):\n",
    "    '''\n",
    "    calculates the frequency spectrum of a hanning window for n samples leading to n frequencies and T window size\n",
    "    n has to be integer and n % 2 has to be 0\n",
    "    '''\n",
    "    assert n%2==0\n",
    "    w = np.hanning(T)\n",
    "    w = w / w.sum()\n",
    "    freq_hann = np.fft.fftfreq(n)\n",
    "\n",
    "    w_transform = np.abs(np.fft.fft(w, n=n))\n",
    "    mid_index = int(n/2)\n",
    "    amp_hann = np.concatenate((w_transform[mid_index:],w_transform[:mid_index]))\n",
    "    freq_hann = np.concatenate((freq_hann[mid_index:],freq_hann[:mid_index]))\n",
    "    return freq_hann,amp_hann\n",
    "\n",
    "def butterworth_window(n,fc,order):\n",
    "    '''\n",
    "    calculates the frequency spectrum of a butterworth window for n samples leading to n frequencies and fc cut off frequency with order of butterworth filter \n",
    "    n has to be integer and n % 2 has to be 0\n",
    "    fs is sample frequency in Hz, we have hourly data, so 1/3600\n",
    "    '''\n",
    "    \n",
    "    assert n%2==0\n",
    "    fs = 1/3600\n",
    "    nyq = 0.5 * fs\n",
    "    fc = fc * (1/3600)\n",
    "    freqs = (-n//2 + np.arange(n))/n/3600\n",
    "\n",
    "    b,a = scipy.signal.butter(N=order,Wn=fc,btype='lowpass',fs = fs)\n",
    "    freq_but, amp_but = scipy.signal.freqz(b,a,fs=fs,worN=freqs)\n",
    "    return freq_but*3600, amp_but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4189f15f-844d-4e9b-b6ad-bdcba723d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na(data):\n",
    "    '''\n",
    "    drops all nan values from 1D array\n",
    "    '''\n",
    "    mask = ~np.isnan(data)\n",
    "    return data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718aaac0-c880-4a47-a918-042e1c6bba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_nufft(t,f_k):\n",
    "    '''\n",
    "    inverse fourier transform to get back time series from fourier components\n",
    "    non-uniform discrete fourier transform\n",
    "    \n",
    "    returns array of reconstructed velocities on the complete time grid, so with nan values\n",
    "    '''\n",
    "    reconstructed = nfft.nfft(drop_na(t), f_k)/len(f_k)\n",
    "    reconstructed_ontime = np.copy(t)\n",
    "    reconstructed_ontime[~np.isnan(t)] = reconstructed\n",
    "    return reconstructed_ontime\n",
    "\n",
    "def inverse_nufft_window(t,f_k,window_amplitudes):\n",
    "    '''\n",
    "    inverse fourier transform to get back time series from fourier components\n",
    "    non-uniform discrete fourier transform but with amplitude taken out of high frequencies using a hanning window, which has to be predefined\n",
    "    \n",
    "    returns array of reconstructed velocities on the complete time grid, so with nan values\n",
    "    '''\n",
    "    reconstructed = nfft.nfft(drop_na(t), f_k*window_amplitudes)/len(f_k)\n",
    "    reconstructed_ontime = np.copy(t)\n",
    "    reconstructed_ontime[~np.isnan(t)] = reconstructed\n",
    "    return reconstructed_ontime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feaae51b-c1e2-4e18-8ed9-26896ab32d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_window(spectral_data,window_type,**parameters):\n",
    "    '''\n",
    "    apply inverse nufft on 2D xarray dataset with a frequency filter applied\n",
    "    returns xarray DataArray of reconstructed filtered velocities on each point in space\n",
    "    '''\n",
    "    if window_type == 'hanning':\n",
    "        freq,amp = hann_window(len(spectral_data.freq),parameters['T'])\n",
    "    elif window_type == 'butterworth':\n",
    "        freq,amp = butterworth_window(len(spectral_data.freq),parameters['fc'],parameters['order'])\n",
    "        \n",
    "    dt = xr.apply_ufunc(\n",
    "                inverse_nufft_window,\n",
    "                spectral_data.t,\n",
    "                spectral_data.f_k,\n",
    "                amp,\n",
    "                input_core_dims=[['freq'],['freq'],['time']],\n",
    "                output_core_dims=[['time']],\n",
    "                vectorize=True,\n",
    "                output_dtypes=[np.dtype(float)],\n",
    "                dask=\"parallelized\",\n",
    "            )\n",
    "    dt = dt.transpose('time','y','x').assign_coords({'time':velocity.time.values})\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f1ac21c-a117-4419-8a68-cf45e29cd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    '''\n",
    "    finds value and idx of value in array that is closest to the looked for value\n",
    "    '''\n",
    "    array = np.asarray(array)\n",
    "    idx = np.unravel_index(np.abs(array - value).argmin(), array.shape)\n",
    "    return array[idx],idx\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r\n",
    "\n",
    "def unique(sequence):\n",
    "    '''\n",
    "    finds unique values of array while keeping the order of values in array\n",
    "    (set() orders values in increasing way)\n",
    "    '''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "def section_indices(data,point1,point2):\n",
    "    '''\n",
    "    get x,y indices of section between latlon point1 and point2\n",
    "    \n",
    "    uses haversine distance to look for closest points on latlon grid to given latlon point\n",
    "    \n",
    "    first defines straight latlon line between point1 and point2\n",
    "    \n",
    "    then looks for closest points on grids to this line\n",
    "    \n",
    "    straight line has higher resolution than grid, so we discard all the x,y points that appear twice\n",
    "    '''\n",
    "    lat = np.asarray(data.lat.data)\n",
    "    lon = np.asarray(data.lon.data)\n",
    "    \n",
    "    if len(lat.shape) == 1 & len(lon.shape) == 1:\n",
    "        lon,lat = np.meshgrid(lon,lat)\n",
    "    \n",
    "    lat1,lon1 = point1\n",
    "    lat2,lon2 = point2\n",
    "    \n",
    "\n",
    "    f = np.frompyfunc(haversine,4,1)\n",
    "    value, indices1 = find_nearest(f(lon,lat,lon1,lat1).astype(float),0)\n",
    "    value, indices2 = find_nearest(f(lon,lat,lon2,lat2).astype(float),0)\n",
    "\n",
    "    x_dist = np.abs(indices1[1] - indices2[1]) \n",
    "    y_dist = np.abs(indices1[0] - indices2[0]) \n",
    "\n",
    "    dist_max = max(x_dist,y_dist)\n",
    "\n",
    "    if (lat1-lat2) == 0:\n",
    "        longitudes = np.linspace(lon1, lon2, dist_max*2)\n",
    "        latitudes = np.ones(dist_max*2)*lat1\n",
    "    elif (lon1-lon2) == 0:\n",
    "        latitudes = np.linspace(lat1, lat2, dist_max*2)\n",
    "        longitudes = np.ones(dist_max*2)*lon1\n",
    "    else:\n",
    "        latitudes = np.linspace(lat1, lat2, dist_max*2)\n",
    "        longitudes = (lon2 - lon1)/(lat2 - lat1)*(latitudes - lat1) + lon1\n",
    "\n",
    "    # x_idx = []\n",
    "    # y_idx = []\n",
    "    indices = []\n",
    "    \n",
    "    for i in range(dist_max*2):\n",
    "        dummy = find_nearest(f(lon,lat,longitudes[i],latitudes[i]).astype(float),0)[1]\n",
    "        # x_idx.append(dummy[1])\n",
    "        # y_idx.append(dummy[0])\n",
    "        indices.append((dummy[1],dummy[0]))\n",
    "    indices = unique(indices)\n",
    "    x_idx = [x[0] for x in indices]\n",
    "    y_idx = [x[1] for x in indices]\n",
    "    \n",
    "    return x_idx,y_idx,latitudes,longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16bdcce5-8d9f-4770-bf21-a70f08f558d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_distance(data):\n",
    "    '''\n",
    "    calculates haversine distance along section from most east point to west in increasing way\n",
    "    here it is the distance from the Taiwan coast, if you set the starting point correctly\n",
    "    '''\n",
    "    distance = [0]\n",
    "    for i in -np.arange(2,len(data.lon.data)+1):\n",
    "        distance.append(haversine(data.lon.data[i],data.lat.data[i],data.lon.data[i+1],data.lat.data[i+1]))\n",
    "    section_distance = np.cumsum(distance)\n",
    "    return np.flip(section_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52734864-c69e-4299-bdc2-f80be700b1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_section(data,point1,point2):\n",
    "    '''\n",
    "    extracts section on latlon grid between point1 and point2\n",
    "    adds distance along section as coordinate\n",
    "    '''\n",
    "    x_idx,y_idx,latitudes,longitudes = section_indices(data,point1,point2)\n",
    "    if 'x' in data.dims:\n",
    "        section = data.isel(x=xr.DataArray(x_idx, dims=\"s\"), y=xr.DataArray(y_idx, dims=\"s\"))\n",
    "    elif 'lat' in data.dims:\n",
    "        section = data.isel(lon=xr.DataArray(x_idx, dims=\"s\"), lat=xr.DataArray(y_idx, dims=\"s\"))\n",
    "        \n",
    "    section = section.assign_coords({'distance':('s',section_distance(section))})\n",
    "    return section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fc76657-5542-4ba7-861a-8ad475d9b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_initial_compass_bearing(pointA, pointB):\n",
    "    \"\"\"\n",
    "    Calculates the bearing between two points.\n",
    "    The formulae used is the following:\n",
    "        θ = atan2(sin(Δlong).cos(lat2),\n",
    "                  cos(lat1).sin(lat2) − sin(lat1).cos(lat2).cos(Δlong))\n",
    "    :Parameters:\n",
    "      - `pointA: The tuple representing the latitude/longitude for the\n",
    "        first point. Latitude and longitude must be in decimal degrees\n",
    "      - `pointB: The tuple representing the latitude/longitude for the\n",
    "        second point. Latitude and longitude must be in decimal degrees\n",
    "    :Returns:\n",
    "      The bearing in degrees\n",
    "    :Returns Type:\n",
    "      float\n",
    "    \"\"\"\n",
    "    if (type(pointA) != tuple) or (type(pointB) != tuple):\n",
    "        raise TypeError(\"Only tuples are supported as arguments\")\n",
    "\n",
    "    lat1 = math.radians(pointA[0])\n",
    "    lat2 = math.radians(pointB[0])\n",
    "\n",
    "    diffLong = math.radians(pointB[1] - pointA[1])\n",
    "\n",
    "    x = math.sin(diffLong) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1)\n",
    "            * math.cos(lat2) * math.cos(diffLong))\n",
    "\n",
    "    initial_bearing = math.atan2(x, y)\n",
    "\n",
    "    # Now we have the initial bearing but math.atan2 return values\n",
    "    # from -180° to + 180° which is not what we want for a compass bearing\n",
    "    # The solution is to normalize the initial bearing as shown below\n",
    "    initial_bearing = math.degrees(initial_bearing)\n",
    "    compass_bearing = (initial_bearing + 360) % 360\n",
    "\n",
    "    return compass_bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eed8ec3d-980d-486d-b2d4-0e5c4640c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_velocities(u,v,angle):\n",
    "    '''\n",
    "    rotates velocity vector in mathematical positive direction\n",
    "    '''\n",
    "    \n",
    "    u_ = u * np.cos(angle) - v * np.sin(angle)\n",
    "    v_ = u * np.sin(angle) + v * np.cos(angle)\n",
    "    \n",
    "    return u_,v_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98850202-5e2f-4dba-811d-29134b298b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_section(section,variables):\n",
    "    '''\n",
    "    rotate velocities on section to cross and along section velocities\n",
    "    \n",
    "    !!! This heavily depends on how you defined the section !!!\n",
    "    \n",
    "    The initial rotation angle is calculated as the compass bearing between two latlon points. I then substract 90deg and change the sign.\n",
    "    This leads to a clockwise rotation with the angle being between the cartesian axis and the section. However, the\n",
    "    initinal angle depends on which one is entered first, leading to a difference of 180deg.\n",
    "    So check the compass bearing between your point first and think about if this leads to the velocites ending up in the way you want.\n",
    "    I wanted u to be mostly eastward and v to be mostly northward\n",
    "    \n",
    "    '''\n",
    "    point1 = (section.isel(s=0).lat.data,section.isel(s=0).lon.data)\n",
    "    point2 = (section.isel(s=-1).lat.data,section.isel(s=-1).lon.data)\n",
    "    angle = np.deg2rad(calculate_initial_compass_bearing(point1,point2))\n",
    "    angle = angle = -(angle- np.pi/2)\n",
    "    section['along_vel'],section['cross_vel'] = rotate_velocities(section[variables[0]],section[variables[1]],angle)\n",
    "    return section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adab45f8-d442-4993-9a6d-7bcdf589622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09c66a03-53c3-4aff-851c-5aab277087e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_list(datau1,datav1,datau2,datav2):\n",
    "    v_1 = [(u,v) for u,v in zip(datau1,datav1)]\n",
    "    v_2 = [(u,v) for u,v in zip(datau2,datav2)]\n",
    "\n",
    "    return np.array(list(map(angle_between,v_1,v_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "596fd8cc-1893-4dd6-b8b3-faf3788a995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_vectors_xarray(data,variables):\n",
    "    u1,v1,u2,v2 = variables\n",
    "    dt = xr.apply_ufunc(angle_between_list,\n",
    "                        data[u1],\n",
    "                        data[v1],\n",
    "                        data[u2],\n",
    "                        data[v2],\n",
    "                        input_core_dims=[['time'],['time'],['time'],['time']],\n",
    "                        output_core_dims=[['time']],\n",
    "                        vectorize=True\n",
    "    )\n",
    "    # dt = dt.transpose(['time','lat','lon'])\n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4de9df86-05bb-4a57-9784-f6031cdb0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(data1,data2):\n",
    "    # data1 is the spatial field you want to correlate to\n",
    "    # data2 is your single time series\n",
    "    # calculates the correlation coefficient and p_value\n",
    "    # returns the result as a numpy array, because the initial output of the function is of a weird PearsonRResult class, which doesnt work in apply_ufunc\n",
    "    result = stats.pearsonr(data1,data2)\n",
    "    return np.stack((result[0],result[1]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15fa60d-d646-47b4-ae18-40328a2827ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
